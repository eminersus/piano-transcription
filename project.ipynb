{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ffmpeg\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Create a directory to store the frames\n",
    "output_image_dir = \"output_frames\"\n",
    "if os.path.exists(output_image_dir):\n",
    "    for file in os.listdir(output_image_dir):\n",
    "        file_path = os.path.join(output_image_dir, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.unlink(file_path)\n",
    "os.makedirs(output_image_dir, exist_ok=True)\n",
    "\n",
    "# Function to find the two strongest horizontal lines in a single frame\n",
    "def find_two_strongest_horizontal_lines(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)  # Apply Gaussian blur before edge detection\n",
    "    edges = cv2.Canny(blurred, 30, 100, apertureSize=3)  # Lowered thresholds for more sensitivity\n",
    "    lines = cv2.HoughLines(edges, 1, np.pi / 180, 200)  # Reduced threshold for more lines\n",
    "\n",
    "    top = 0\n",
    "    bottom = frame.shape[0]\n",
    "\n",
    "    if lines is not None:\n",
    "        strongest_lines = []\n",
    "        for rho, theta in lines[:, 0]:\n",
    "            theta_diff = abs(theta - np.pi / 2)\n",
    "            strongest_lines.append((theta_diff, rho, theta))\n",
    "\n",
    "        strongest_lines = sorted(strongest_lines, key=lambda x: x[0])[:2]\n",
    "\n",
    "        if len(strongest_lines) == 2:\n",
    "            y_coords = []\n",
    "            for _, rho, theta in strongest_lines:\n",
    "                a = np.cos(theta)\n",
    "                b = np.sin(theta)\n",
    "                y0 = b * rho\n",
    "                y_coords.append(int(y0))\n",
    "\n",
    "            y_coords.sort()\n",
    "            if len(y_coords) == 2 and y_coords[0] < y_coords[1]:\n",
    "                top = max(0, y_coords[0] - 10)\n",
    "                bottom = min(frame.shape[0], y_coords[1] + 10)\n",
    "\n",
    "    return top, bottom\n",
    "\n",
    "# Function to segment white and black keys using binary masks\n",
    "def segment_piano_keys(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Threshold to create a binary mask for white keys\n",
    "    _, white_key_mask = cv2.threshold(gray, 100, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Threshold to create a binary mask for black keys (invert white keys mask)\n",
    "    black_key_mask = cv2.bitwise_not(white_key_mask)\n",
    "\n",
    "    return white_key_mask, black_key_mask\n",
    "\n",
    "# Function for seeded segmentation based on black key positions\n",
    "def seeded_segmentation(black_key_mask, frame):\n",
    "    # Find contours in the black key mask\n",
    "    contours, _ = cv2.findContours(black_key_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    seeds = []\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        # Center of the black key\n",
    "        seed_center = (x + w // 2, y + h // 2)\n",
    "\n",
    "        # Ensure the seed is within the contour\n",
    "        if cv2.pointPolygonTest(contour, seed_center, False) >= 0:\n",
    "            seeds.append(seed_center)\n",
    "\n",
    "        # Left and right of the black key\n",
    "        seed_left = (x, y + h // 2)\n",
    "        seed_right = (x + w, y + h // 2)\n",
    "\n",
    "        if cv2.pointPolygonTest(contour, seed_left, False) >= 0:\n",
    "            seeds.append(seed_left)\n",
    "        if cv2.pointPolygonTest(contour, seed_right, False) >= 0:\n",
    "            seeds.append(seed_right)\n",
    "\n",
    "    # Visualize seeds on the frame\n",
    "    for seed in seeds:\n",
    "        cv2.circle(frame, seed, 5, (0, 255, 0), -1)  # Draw seeds as green circles\n",
    "\n",
    "    # Create a labeled image for seeded segmentation\n",
    "    labeled_mask = np.zeros_like(black_key_mask, dtype=np.int32)\n",
    "    for i, seed in enumerate(seeds):\n",
    "        if 0 <= seed[0] < black_key_mask.shape[1] and 0 <= seed[1] < black_key_mask.shape[0]:\n",
    "            labeled_mask[seed[1], seed[0]] = i + 1\n",
    "\n",
    "    # Perform watershed algorithm for segmentation\n",
    "    gray = cv2.cvtColor(black_key_mask, cv2.COLOR_GRAY2BGR)\n",
    "    distance_transform = cv2.distanceTransform(black_key_mask, cv2.DIST_L2, 5)\n",
    "    _, sure_fg = cv2.threshold(distance_transform, 0.9 * distance_transform.max(), 255, 0)\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    unknown = cv2.subtract(black_key_mask, sure_fg)\n",
    "\n",
    "    markers = cv2.connectedComponents(sure_fg)[1]\n",
    "    markers = markers + 1\n",
    "    markers[unknown == 255] = 0\n",
    "\n",
    "    segmented = cv2.watershed(gray, markers)\n",
    "\n",
    "    # Overlay the segmented regions on the original frame\n",
    "    frame[segmented == -1] = [0, 0, 255]  # Mark boundaries in red\n",
    "\n",
    "    return frame\n",
    "\n",
    "# Input video file\n",
    "input_video_path = \"dataset/MIDItest/miditest_videos/5.mp4\"\n",
    "\n",
    "# Probe video dimensions\n",
    "probe = ffmpeg.probe(input_video_path)\n",
    "video_stream = next(stream for stream in probe['streams'] if stream['codec_type'] == 'video')\n",
    "width = int(video_stream['width'])\n",
    "height = int(video_stream['height'])\n",
    "\n",
    "# Read the first frame to determine cropping boundaries\n",
    "process = (\n",
    "    ffmpeg.input(input_video_path)\n",
    "    .output('pipe:', format='rawvideo', pix_fmt='bgr24')\n",
    "    .run_async(pipe_stdout=True)\n",
    ")\n",
    "\n",
    "in_bytes = process.stdout.read(width * height * 3)\n",
    "frame = np.frombuffer(in_bytes, np.uint8).reshape([height, width, 3])\n",
    "\n",
    "top, bottom = find_two_strongest_horizontal_lines(frame)\n",
    "process.stdout.close()\n",
    "process.wait()\n",
    "\n",
    "# Read video frames again and save each frame as an image\n",
    "process = (\n",
    "    ffmpeg.input(input_video_path)\n",
    "    .output('pipe:', format='rawvideo', pix_fmt='bgr24')\n",
    "    .run_async(pipe_stdout=True)\n",
    ")\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    in_bytes = process.stdout.read(width * height * 3)\n",
    "    if not in_bytes:\n",
    "        break\n",
    "\n",
    "    frame = np.frombuffer(in_bytes, np.uint8).reshape([height, width, 3])\n",
    "\n",
    "    # Crop the frame\n",
    "    cropped_frame = frame[top:bottom, :]\n",
    "    \n",
    "    # Create a writable copy of the cropped frame\n",
    "    cropped_frame = cropped_frame.copy()\n",
    "\n",
    "    # Segment piano keys in the cropped frame\n",
    "    white_key_mask, black_key_mask = segment_piano_keys(cropped_frame)\n",
    "\n",
    "    # Perform seeded segmentation\n",
    "    segmented_frame = seeded_segmentation(black_key_mask, cropped_frame)\n",
    "\n",
    "    # Save the segmented frame\n",
    "    segmented_path = os.path.join(output_image_dir, f\"segmented_frame_{frame_count:04d}.jpg\")\n",
    "    cv2.imwrite(segmented_path, segmented_frame)\n",
    "\n",
    "    frame_count += 1\n",
    "    print(f\"Saved {segmented_path}\")\n",
    "\n",
    "# Cleanup\n",
    "process.stdout.close()\n",
    "process.wait()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def midi_to_piano_vector(file_path):\n",
    "    # Load the MIDI file\n",
    "    midi_data = pretty_midi.PrettyMIDI(file_path)\n",
    "\n",
    "    # Define the range of piano notes (A0 to C8 corresponds to MIDI notes 21 to 108)\n",
    "    piano_range = range(21, 109)\n",
    "\n",
    "    # Collect all note events\n",
    "    note_events = []\n",
    "    for instrument in midi_data.instruments:\n",
    "        for note in instrument.notes:\n",
    "            # Normalize the velocity to the range [0, 1]\n",
    "            normalized_velocity = note.velocity / 127.0\n",
    "            note_name = pretty_midi.note_number_to_name(note.pitch)\n",
    "            note_events.append((note.start, note.pitch, normalized_velocity, 'on', note_name))  # Note-on event\n",
    "            note_events.append((note.end, note.pitch, normalized_velocity, 'off', note_name))  # Note-off event\n",
    "\n",
    "    # Sort events by time\n",
    "    note_events.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Create a list to store the output\n",
    "    piano_vectors = []\n",
    " \n",
    "    # Initialize the state of the piano (88-dimensional vector)\n",
    "    piano_state = np.zeros(88, dtype=float)\n",
    "\n",
    "    # Process each event\n",
    "    for event in note_events:\n",
    "        timestamp, pitch, velocity, event_type, note_name = event\n",
    "\n",
    "        if pitch in piano_range:\n",
    "            note_index = pitch - 21  # Map pitch to the 88-key piano range\n",
    "\n",
    "            if event_type == 'on':\n",
    "                piano_state[note_index] = velocity  # Set the normalized velocity\n",
    "            elif event_type == 'off':\n",
    "                piano_state[note_index] = 0  # Release the note\n",
    "\n",
    "        # Shift timestamp by 0.6 seconds\n",
    "        shifted_timestamp = timestamp - 0.25\n",
    "\n",
    "        # Append the current state, shifted timestamp, and note name\n",
    "        piano_vectors.append({\"timestamp\": shifted_timestamp, \"vector\": piano_state.tolist(), \"note_name\": note_name})\n",
    "\n",
    "    return piano_vectors, midi_data.get_end_time()\n",
    "\n",
    "# Example usage\n",
    "def save_output_to_file(midi_file, output_file):\n",
    "    output, midi_duration = midi_to_piano_vector(midi_file)\n",
    "    \n",
    "    # Include the total duration in the output\n",
    "    result = {\n",
    "        \"duration\": midi_duration,\n",
    "        \"piano_vectors\": output\n",
    "    }\n",
    "\n",
    "    # Output the result to a JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(result, f, indent=4)\n",
    "\n",
    "# Replace with your MIDI file path and desired output file path\n",
    "midi_file = '/Users/tunaonat/Desktop/proje-git/piano-transcription/dataset/MIDItest/miditest_MIDI/5.mid'\n",
    "output_file = 'output.json'\n",
    "save_output_to_file(midi_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ffmpeg\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Create a directory to store the difference frames\n",
    "output_diff_dir = \"output_differences\"\n",
    "if os.path.exists(output_diff_dir):\n",
    "    for file in os.listdir(output_diff_dir):\n",
    "        file_path = os.path.join(output_diff_dir, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.unlink(file_path)\n",
    "os.makedirs(output_diff_dir, exist_ok=True)\n",
    "\n",
    "# Function to find the two strongest horizontal lines in a single frame\n",
    "def find_two_strongest_horizontal_lines(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    edges = cv2.Canny(blurred, 30, 100, apertureSize=3)\n",
    "    lines = cv2.HoughLines(edges, 1, np.pi / 180, 200)\n",
    "\n",
    "    top = 0\n",
    "    bottom = frame.shape[0]\n",
    "\n",
    "    if lines is not None:\n",
    "        strongest_lines = []\n",
    "        for rho, theta in lines[:, 0]:\n",
    "            theta_diff = abs(theta - np.pi / 2)\n",
    "            strongest_lines.append((theta_diff, rho, theta))\n",
    "\n",
    "        strongest_lines = sorted(strongest_lines, key=lambda x: x[0])[:2]\n",
    "\n",
    "        if len(strongest_lines) == 2:\n",
    "            y_coords = []\n",
    "            for _, rho, theta in strongest_lines:\n",
    "                a = np.cos(theta)\n",
    "                b = np.sin(theta)\n",
    "                y0 = b * rho\n",
    "                y_coords.append(int(y0))\n",
    "\n",
    "            y_coords.sort()\n",
    "            if len(y_coords) == 2 and y_coords[0] < y_coords[1]:\n",
    "                top = max(0, y_coords[0] - 10)\n",
    "                bottom = min(frame.shape[0], y_coords[1] + 10)\n",
    "\n",
    "    return top, bottom\n",
    "\n",
    "# Input video file\n",
    "input_video_path = \"/Users/tunaonat/Desktop/proje-git/piano-transcription/dataset/MIDItest/miditest_videos/5.mp4\"\n",
    "\n",
    "# Probe video dimensions\n",
    "probe = ffmpeg.probe(input_video_path)\n",
    "video_stream = next(stream for stream in probe['streams'] if stream['codec_type'] == 'video')\n",
    "width = int(video_stream['width'])\n",
    "height = int(video_stream['height'])\n",
    "fps = eval(video_stream['r_frame_rate'])  # Get FPS as a float\n",
    "\n",
    "# Read the first frame to determine cropping boundaries\n",
    "process = (\n",
    "    ffmpeg.input(input_video_path)\n",
    "    .output('pipe:', format='rawvideo', pix_fmt='bgr24')\n",
    "    .run_async(pipe_stdout=True)\n",
    ")\n",
    "\n",
    "in_bytes = process.stdout.read(width * height * 3)\n",
    "frame = np.frombuffer(in_bytes, np.uint8).reshape([height, width, 3])\n",
    "\n",
    "top, bottom = find_two_strongest_horizontal_lines(frame)\n",
    "process.stdout.close()\n",
    "process.wait()\n",
    "\n",
    "# Read video frames again and calculate differences\n",
    "process = (\n",
    "    ffmpeg.input(input_video_path)\n",
    "    .output('pipe:', format='rawvideo', pix_fmt='bgr24')\n",
    "    .run_async(pipe_stdout=True)\n",
    ")\n",
    "\n",
    "prev_frame = None\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    in_bytes = process.stdout.read(width * height * 3)\n",
    "    if not in_bytes:\n",
    "        break\n",
    "\n",
    "    frame = np.frombuffer(in_bytes, np.uint8).reshape([height, width, 3])\n",
    "\n",
    "    # Crop the frame\n",
    "    cropped_frame = frame[top:bottom, :]\n",
    "\n",
    "    # Convert to grayscale for difference calculation\n",
    "    gray_frame = cv2.cvtColor(cropped_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    if prev_frame is not None:\n",
    "        # Compute absolute difference\n",
    "        diff = cv2.absdiff(prev_frame, gray_frame)\n",
    "\n",
    "        # Generate timestamp\n",
    "        timestamp = frame_count / fps\n",
    "        timestamp_text = f\"{timestamp:.2f}s\"\n",
    "\n",
    "        # Save the difference frame with timestamp in the filename\n",
    "        diff_path = os.path.join(output_diff_dir, f\"frame_diff_{timestamp_text}_{frame_count:04d}.jpg\")\n",
    "        cv2.imwrite(diff_path, diff)\n",
    "\n",
    "        print(f\"Saved {diff_path}\")\n",
    "\n",
    "    # Update previous frame\n",
    "    prev_frame = gray_frame\n",
    "    frame_count += 1\n",
    "\n",
    "# Cleanup\n",
    "process.stdout.close()\n",
    "process.wait()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Load Preprocessed Frames\n",
    "\n",
    "def load_frames(output_folder):\n",
    "    frames = []\n",
    "    frame_timestamps = []\n",
    "    frame_files = sorted(os.listdir(output_folder))  # Ensure frames are sorted by time\n",
    "\n",
    "    for file in frame_files:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            frame_path = os.path.join(output_folder, file)\n",
    "            frame = cv2.imread(frame_path, cv2.IMREAD_GRAYSCALE)\n",
    "            height, width = frame.shape\n",
    "\n",
    "            # Split the frame into top and bottom halves and stack them\n",
    "            top_half = frame[:height // 2, :]\n",
    "            bottom_half = frame[height // 2:, :]\n",
    "            stacked_frame = np.vstack((top_half, bottom_half))\n",
    "\n",
    "            frames.append(stacked_frame)\n",
    "\n",
    "            # Extract timestamp from filename\n",
    "            timestamp = float(file.split(\"_\")[2][:-1])  # Extract the seconds value (e.g., 0.24s)\n",
    "            frame_timestamps.append(timestamp)\n",
    "\n",
    "    return np.array(frames), np.array(frame_timestamps)\n",
    "\n",
    "# Step 2: Resize Frames and Save to Folder\n",
    "\n",
    "def save_resized_frames(frames, target_size, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for idx, frame in enumerate(frames):\n",
    "        resized_frame = cv2.resize(frame, target_size)\n",
    "        output_path = os.path.join(output_folder, f\"frame_{idx:04d}.jpg\")\n",
    "        cv2.imwrite(output_path, resized_frame)\n",
    "\n",
    "# Step 3: Load JSON Data and Synchronize\n",
    "\n",
    "def load_labels(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    label_timestamps = [entry['timestamp'] for entry in data['piano_vectors']]\n",
    "    vectors = [entry['vector'] for entry in data['piano_vectors']]\n",
    "    return np.array(label_timestamps), np.array(vectors)\n",
    "\n",
    "\n",
    "def synchronize_data(frames, frame_timestamps, label_timestamps, vectors):\n",
    "    synchronized_frames = []\n",
    "    synchronized_vectors = []\n",
    "\n",
    "    for label_timestamp in label_timestamps:\n",
    "        # Find the three closest frame timestamps\n",
    "        closest_indices = np.argsort(np.abs(frame_timestamps - label_timestamp))[:3]\n",
    "\n",
    "        # Stack the three closest frames as channels\n",
    "        selected_frames = np.stack([frames[idx] for idx in closest_indices], axis=-1)\n",
    "\n",
    "        # Append the synchronized data\n",
    "        synchronized_frames.append(selected_frames)\n",
    "        synchronized_vectors.append(vectors[np.argmin(np.abs(label_timestamps - label_timestamp))])\n",
    "\n",
    "    return np.array(synchronized_frames), np.array(synchronized_vectors)\n",
    "\n",
    "# Step 4: Prepare Dataset\n",
    "\n",
    "class PianoDataset(Dataset):\n",
    "    def __init__(self, frames, vectors, target_size=(64, 64)):\n",
    "        resized_frames = [cv2.resize(frame, target_size) for frame in frames]\n",
    "        self.frames = torch.tensor(resized_frames, dtype=torch.float32).permute(0, 3, 1, 2) / 255.0  # Normalize and rearrange\n",
    "        self.vectors = torch.tensor(vectors, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.frames[idx], self.vectors[idx]\n",
    "\n",
    "# Step 5: Define CNN Model\n",
    "\n",
    "class PianoCNN(nn.Module):\n",
    "    def __init__(self, input_channels, output_dim):\n",
    "        super(PianoCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Calculate input size dynamically\n",
    "        dummy_input = torch.zeros(1, input_channels, 64, 64)  # Assuming resized input to 64x64\n",
    "        with torch.no_grad():\n",
    "            dummy_output = self.pool2(self.pool1(self.conv2(self.pool1(self.conv1(dummy_input)))))\n",
    "        fc1_input_dim = dummy_output.numel()\n",
    "\n",
    "        self.fc1 = nn.Linear(fc1_input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Step 6: Train and Evaluate Model\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, device, epochs=10):\n",
    "    model = model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(dataloader)}\")\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths to output folder and JSON file\n",
    "    output_folder = \"output_differences\"\n",
    "    json_path = \"output.json\"\n",
    "    resized_folder = \"cnn_input\"\n",
    "\n",
    "    # Load preprocessed frames\n",
    "    frames, frame_timestamps = load_frames(output_folder)\n",
    "    frame_height, frame_width = frames[0].shape[:2]  # Automatically detect frame dimensions\n",
    "\n",
    "    # Save resized frames\n",
    "    save_resized_frames(frames, target_size=(64, 64), output_folder=resized_folder)\n",
    "\n",
    "    # Load labels and synchronize\n",
    "    label_timestamps, vectors = load_labels(json_path)\n",
    "    synchronized_frames, synchronized_vectors = synchronize_data(frames, frame_timestamps, label_timestamps, vectors)\n",
    "\n",
    "    # Prepare dataset\n",
    "    dataset = PianoDataset(synchronized_frames, synchronized_vectors, target_size=(64, 64))\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Define model, criterion, and optimizer\n",
    "    input_channels = 3  # Three stacked frames\n",
    "    output_dim = synchronized_vectors.shape[1]\n",
    "    model = PianoCNN(input_channels, output_dim)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_model(model, dataloader, criterion, optimizer, device, epochs=10)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), \"piano_key_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
